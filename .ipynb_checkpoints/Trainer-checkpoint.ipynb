{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.22:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1575583653279)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, IDF, OneHotEncoderEstimator, RegexTokenizer, StringIndexer}\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, TrainValidationSplitModel}\n",
       "import java.io.File\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, IDF, OneHotEncoderEstimator, RegexTokenizer, StringIndexer}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, TrainValidationSplitModel}\n",
    "\n",
    "import java.io.File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "//                    TP 3 : Machine learning avec Spark                           //\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "//                    Chargement des donnees                                       //\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "\n",
      "Training Dataframe\n",
      "+--------------+--------------------+--------------------+-----+--------------------+------------+--------+---------+-------------+-----------+--------------------+\n",
      "|    project_id|                name|                desc| goal|            keywords|final_status|country2|currency2|days_campaign|hours_prepa|                text|\n",
      "+--------------+--------------------+--------------------+-----+--------------------+------------+--------+---------+-------------+-----------+--------------------+\n",
      "| kkst106359630|matthew francis a...|a new ep from a m...| 1000|matthew-francis-a...|           1|      US|      USD|           14|    821.123|matthew francis a...|\n",
      "|kkst1504658925|launch bossfm - d...|a group of millen...| 5000|launch-bossfm-dig...|           0|      US|      USD|           30|    566.332|launch bossfm - d...|\n",
      "|kkst1417129849|iron horse tv = m...|high energy reali...|99000|iron-horse-tv-mus...|           0|      US|      USD|           40|     158.13|iron horse tv = m...|\n",
      "| kkst344397801|treachery in beat...|use a mix of braw...|49000|treachery-in-beat...|           0|      US|      USD|           30|     341.14|treachery in beat...|\n",
      "|kkst1579286256|              piplay|the raspberry pi ...| 2500|              pimame|           1|      US|      USD|           30|   5352.849|piplay the raspbe...|\n",
      "+--------------+--------------------+--------------------+-----+--------------------+------------+--------+---------+-------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "()\n",
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      " |-- country2: string (nullable = true)\n",
      " |-- currency2: string (nullable = true)\n",
      " |-- days_campaign: integer (nullable = true)\n",
      " |-- hours_prepa: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "//                    Utilisation des donnees textuelles                           //\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "//                    Etape 1 : Separation des textes en mots                      //\n",
      "//                    Etape 2 : Retirage des stop words                            //\n",
      "//                    Etape 3 : Conversion en TF-IDF                               //\n",
      "//                    Etape 4 : Recherche de la partie IDF                         //\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "//                    Conversion des variables catégorielles en variables numeriques/\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "//                    Etape 1 : Conversion de country2                             //\n",
      "//                    Etape 2 : Conversion de currency2                            //\n",
      "//                    Etape 3 : One-Hot encoder de ces deux catégories             //\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "//                    Mise des donnees sous un forme interpretable par SparkML     //\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "//                    Etape 1 : VectorAssembler                                    //\n",
      "//                    Etape 2 : Creation et instanciation du modèle de classification\n",
      "//                    Regression logistique                                        //\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "//                    Création du Pipeline                                         //\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "//                    Entrainement, test et evaluation du modele                   //\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "Model 1 was fit using parameters: {\n",
      "\tpipeline_82e436be9ec1-stages: [Lorg.apache.spark.ml.PipelineStage;@274fa437\n",
      "}\n",
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0| 1764|\n",
      "|           0|        1.0| 2269|\n",
      "|           1|        1.0| 1666|\n",
      "|           0|        0.0| 5094|\n",
      "+------------+-----------+-----+\n",
      "\n",
      "Le f1-score est de 0.6325079964153939 \n",
      "\n",
      "//                    Entrainement du modèle avec l'échantillon training          //\n",
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0| 1003|\n",
      "|           0|        1.0| 2740|\n",
      "|           1|        1.0| 2427|\n",
      "|           0|        0.0| 4623|\n",
      "+------------+-----------+-----+\n",
      "\n",
      "Le f1 score est de 0.6650475698419106 \n",
      "\n",
      "//                    Evaluation de la precision (accuracy)                       //\n",
      "//                    Obtention de la mesure de performance                       //\n",
      "Precision obtenue : 0.6532011488928009 \n",
      "\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n",
      "//                    Sauvegarde du modele                                         //\n",
      "/////////////////////////////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@6f22b98f\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@36b7eb95\n",
       "df: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 9 more fields]\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_d1e65f7bae62\n",
       "stopWordsRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_53a7cea30c20\n",
       "countVectorizedModel: org.apache.spark.ml.feature.CountVectorizer = cntVec_b01c3e3dc2ab\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_b62022208740\n",
       "stringIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_9d2350cf2982\n",
       "stringIndexer2: org.apache.spark.ml.feature.StringIndexer = strIdx_bdbae834b061\n",
       "oneHotEncoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   val conf = new SparkConf().setAll(Map(\n",
    "      \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "      \"spark.speculation\" -> \"false\",\n",
    "      \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "      \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "      \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "      \"spark.default.parallelism\" -> \"12\",\n",
    "      \"spark.sql.shuffle.partitions\" -> \"12\",\n",
    "      \"spark.driver.maxResultSize\" -> \"2g\"\n",
    "    ))\n",
    "\n",
    "    val spark = SparkSession\n",
    "      .builder\n",
    "      .config(conf)\n",
    "      .appName(\"TP Spark : Trainer\")\n",
    "      .getOrCreate()\n",
    "\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "    println(\"//                    TP 3 : Machine learning avec Spark                           //\")\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "\n",
    "    /*******************************************************************************\n",
    "      *\n",
    "      *       TP 3\n",
    "      *\n",
    "      *       - lire le fichier sauvegarder précédemment\n",
    "      *       - construire les Stages du pipeline, puis les assembler\n",
    "      *       - trouver les meilleurs hyperparamètres pour l'entraînement du pipeline avec une grid-search\n",
    "      *       - Sauvegarder le pipeline entraîné\n",
    "      *\n",
    "      *       if problems with unimported modules => sbt plugins update\n",
    "      *\n",
    "      ********************************************************************************/\n",
    "\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "    println(\"//                    Chargement des donnees                                       //\")\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "    val df: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", value = true) // utilise la première ligne du (des) fichier(s) comme header\n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .parquet(\"src/main/ressources/monDataFrameFinal\") // src/main/ressources/monDataFrameFinal\") //data/prepared_trainingset\")  \n",
    "\n",
    "    println(\"\\nTraining Dataframe\")\n",
    "    println(df.show(5))\n",
    "    df.printSchema\n",
    "\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "    println(\"//                    Utilisation des donnees textuelles                           //\")\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "    println(\"//                    Etape 1 : Separation des textes en mots                      //\")\n",
    "    val tokenizer = new RegexTokenizer()\n",
    "      .setPattern(\"\\\\W+\")\n",
    "      .setGaps(true)\n",
    "      .setInputCol(\"text\")\n",
    "      .setOutputCol(\"tokens\")\n",
    "\n",
    "    println(\"//                    Etape 2 : Retirage des stop words                            //\")\n",
    "    val stopWordsRemover = new StopWordsRemover()\n",
    "      .setInputCol(\"tokens\")\n",
    "      .setOutputCol(\"filtered\")\n",
    "\n",
    "    println(\"//                    Etape 3 : Conversion en TF-IDF                               //\")\n",
    "    val countVectorizedModel = new CountVectorizer()\n",
    "      .setInputCol(\"filtered\")\n",
    "      .setOutputCol(\"vectorized\")\n",
    "\n",
    "    println(\"//                    Etape 4 : Recherche de la partie IDF                         //\")\n",
    "    val idf = new IDF()\n",
    "      .setInputCol(\"vectorized\")\n",
    "      .setOutputCol(\"tfidf\")\n",
    "\n",
    "\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "    println(\"//                    Conversion des variables catégorielles en variables numeriques/\")\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "    println(\"//                    Etape 1 : Conversion de country2                             //\")\n",
    "    val stringIndexer = new StringIndexer()\n",
    "      .setInputCol(\"country2\")\n",
    "      .setOutputCol(\"country_indexed\")\n",
    "      .setHandleInvalid(\"skip\")\n",
    "\n",
    "    println(\"//                    Etape 2 : Conversion de currency2                            //\")\n",
    "    val stringIndexer2 = new StringIndexer()\n",
    "      .setInputCol(\"currency2\")\n",
    "      .setOutputCol(\"currency_indexed\")\n",
    "      .setHandleInvalid(\"skip\")\n",
    "\n",
    "    println(\"//                    Etape 3 : One-Hot encoder de ces deux catégories             //\")\n",
    "    val oneHotEncoder = new OneHotEncoderEstimator() // OneHotEncoder étant deprecated, on utilise OneHotEncoderEstimator\n",
    "      .setInputCols(Array(\"country_indexed\", \"currency_indexed\"))\n",
    "      .setOutputCols(Array(\"country_onehot\", \"currency_onehot\"))\n",
    "\n",
    "\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "    println(\"//                    Mise des donnees sous un forme interpretable par SparkML     //\")\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "    println(\"//                    Etape 1 : VectorAssembler                                    //\")\n",
    "    val vectorAssembler = new VectorAssembler()\n",
    "      .setInputCols(Array(\"tfidf\", \"days_campaign\", \"hours_prepa\", \"goal\", \"country_onehot\", \"currency_onehot\"))\n",
    "      .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "    println(\"//                    Etape 2 : Creation et instanciation du modèle de classification\")\n",
    "    println(\"//                    Regression logistique                                        //\")\n",
    "    val lr = new LogisticRegression()\n",
    "      .setElasticNetParam(0.0)\n",
    "      .setFitIntercept(true)\n",
    "      .setFeaturesCol(\"features\")\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setStandardization(true)\n",
    "      .setPredictionCol(\"predictions\")\n",
    "      .setRawPredictionCol(\"raw_predictions\")\n",
    "      .setThresholds(Array(0.7, 0.3))\n",
    "      .setTol(1.0e-6)\n",
    "      .setMaxIter(20)\n",
    "\n",
    "\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "    println(\"//                    Création du Pipeline                                         //\")\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "    val stages10 = Array(tokenizer, stopWordsRemover, countVectorizedModel, idf, stringIndexer, stringIndexer2, oneHotEncoder, vectorAssembler, lr)\n",
    "    val pipeline = new Pipeline().setStages(stages10)\n",
    "\n",
    "\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "    println(\"//                    Entrainement, test et evaluation du modele                   //\")\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "    val Array(training, test) = df.randomSplit(Array(0.9, 0.1), seed = 1991)\n",
    "\n",
    "    val model = pipeline.fit(training)\n",
    "    println(s\"\\nModel 1 was fit using parameters: ${model.parent.extractParamMap}\")\n",
    "\n",
    "    val dfWithSimplePredictions = model.transform(test)\n",
    "\n",
    "    dfWithSimplePredictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "\n",
    "    val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setMetricName(\"f1\")\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setPredictionCol(\"predictions\")\n",
    "\n",
    "    val f1score = evaluator.evaluate(dfWithSimplePredictions)\n",
    "\n",
    "    println(\"Le f1-score est de \" + f1score + \" \\n\")\n",
    "\n",
    "    val paramGrid = new ParamGridBuilder()\n",
    "      .addGrid(lr.regParam, Array(10e-8, 10e-6, 10e-4, 10e-2))\n",
    "      .addGrid(countVectorizedModel.minDF, Array(55.0, 75.0, 95.0))\n",
    "      .build()\n",
    "\n",
    "    //  TrainValidationSplit requiert un estimateur, un set d'estimateur ParamMaps, et un Evaluator.\n",
    "    val trainValidationSplit = new TrainValidationSplit()\n",
    "      .setEstimator(pipeline)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setTrainRatio(0.7)\n",
    "\n",
    "    println(\"//                    Entrainement du modèle avec l'échantillon training          //\")\n",
    "    val validationModel = trainValidationSplit.fit(training)\n",
    "\n",
    "    val dfWithPredictions = validationModel.transform(test).select(\"features\",\"final_status\",\"predictions\")\n",
    "\n",
    "    dfWithPredictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "\n",
    "    val score = evaluator.evaluate(dfWithPredictions)\n",
    "\n",
    "    println(\"Le f1 score est de \" + score + \" \\n\")\n",
    "\n",
    "\n",
    "    println(\"//                    Evaluation de la precision (accuracy)                       //\")\n",
    "    val evaluator_acc = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setPredictionCol(\"predictions\")\n",
    "      .setMetricName(\"accuracy\")\n",
    "\n",
    "    println(\"//                    Obtention de la mesure de performance                       //\")\n",
    "    val accuracy = evaluator_acc.evaluate(dfWithPredictions)\n",
    "    println(\"\\nPrecision obtenue : \" + accuracy +\" \\n\")\n",
    "\n",
    "\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "    println(\"//                    Sauvegarde du modele                                         //\")\n",
    "    println(\"/////////////////////////////////////////////////////////////////////////////////////\")\n",
    "\n",
    "\n",
    "    validationModel.write.overwrite.save(\"src/main/model/LogisticRegression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
